\input{../header.tex}
\def\COMPILINGFROMMAIN{}
\begin{document}
\title{\Huge{Probabilistic Numerical Solutions of Partial Differential Equations on Riemannian 2-Manifolds}}
\author{
\\
Master's Thesis in Computer Science
\\
\\
\\
\\
\\
\\
\\
Theo Rüter Würtzen\vspace{2mm}
\\
Department of Computer Science
\\
University of Copenhagen
\\
\\
\\
\\
\\
\\
\\
\\
\\
Supervised by 
\\
\\
\\
\\
\\
Peter Nicholas Krämer\vspace{2mm}
\\
Section for Cognitive Systems
\\
Technical University of Denmark
\\
\\
\\
\\
\hspace*{7mm}\begin{tabular}{ccc}
    \begin{tabular}{c}
        Søren Hauberg\vspace{2mm}
\\
Section for Cognitive Systems
\\
Technical University of Denmark
    \end{tabular}
     & & \begin{tabular}{c}
        Sebastian Weichwald\vspace{2mm}
\\
Department of Mathematical Sciences
\\
University of Copenhagen
    \end{tabular}
\end{tabular}
}
\date{}
\maketitle

\clearpage


\begin{abstract}
Partial differential equations are ubiquitous in physics and simulations, and generally have to be solved numerically. The field of probabilistic numerics focuses on framing computation as probabilistic inference and has led to the development of efficient and competitive probabilistic numerical solvers of ordinary differential equations, which give the solution as a stochastic process. In this thesis, we extend the use of these solvers to partial differential equations on Riemannian manifolds. We give introductions to the touched upon topics, including but not limited to discretization of Riemannian manifolds, the associated discrete differential operators, and the use of stochastic differential equations as priors. We give an algorithm to build an intrinsic triangulation of a manifold, and empirically demonstrate how certain priors can accelerate the process of solving nonlinear PDEs. We have structured the thesis as a high level guide on how to implement and follow the results, and we try to motivate the steps to the best of our efforts. We teach and explain with a focus on giving intuition through examples and analogies, and provide pointers to further reading.
\end{abstract}

\section*{Acknowledgements}
I thank Nicholas for his high availability and thorough eye for detail during every supervision session and for including me in the fun sessions at the research group. I thank Søren Hauberg and Sebastian Weichwald for their enthusiasm and for enabling this project. Finally, I pat myself on the back for putting in a lot of work.

\newpage
\tableofcontents
\newpage
\section{Overview}\label{sec:overview}
The thesis can broadly be seen as consisting of two separate parts that meet towards the end. One deals with Riemannian manifolds and how to convert them to a discrete form amenable to numerical computation. The other deals with ordinary / partial / stochastic differential equations and probabilistic numerical solvers. Finally these two parts converge, and we will use the discrete Laplacian and the probabilistic numerical solvers to solve partial differential equations whose spatial domain is the manifold.
\\
The thesis touches upon many subjects, some of which are active areas of research and could have been an entire thesis by themselves. We will supplement what is not taught in a classic computer / data science curriculum, but a full coverage of the topics is beyond the scope of this thesis. Nearly all figures in the thesis were made during the project and have subsequently been included in the thesis because they were found to be helpful in understanding the topics. 
\\The main parts of the thesis are structured as follows:
\\\\
Chapter \ref{sec:background} is the most verbose part of the thesis and serves to get the reader into the mood and right mindset for the rest of the thesis. It gives a brief introduction to the field of probabilistic numerics and further motivates the topics of the thesis.
\\\\
Chapter \ref{sec:manifolds} gives an applied introduction to Riemannian manifolds, triangle meshes and the discrete exterior calculus, all with the intention of defining the discrete Laplacian on a discrete manifold.
\\\\
Chapter \ref{sec:intrinsic_triangulation} builds on chapter \ref{sec:manifolds} and addresses the problem of building the Laplacian matrix given only a Riemannian metric. We contribute an algorithm that solves this problem by constructing an intrinsic triangulation of a manifold, from which one can then build the Laplacian matrix using the tools from \ref{sec:manifolds}. We demonstrate the algorithm on a few examples.
\\\\
Chapter \ref{sec:pde} is the second part of the thesis. It gives an introduction to ordinary and partial differential equations. We discuss the ideas of the state-space representation and discretization using the Method of Lines, which is the principle we will use in the later probabilistic numerical solver.
\\\\
Chapter \ref{sec:prior} builds on chapter \ref{sec:pde} to introduce the probabilistic numerical solver for ordinary differential equations. We will cover how to specify a prior distribution over the solution using a stochastic differential equation, show how to manipulate the prior and posterior probability distributions, and give specific pointers on how to implement a stable solver.
\\\\
Chapter \ref{sec:solver_experiments} wraps up with a demonstration of the solver on a few examples. We show how the solver can be used to solve nonlinear PDEs, and contribute problem-specific priors that improve the speed of convergence.

\section{Background}\label{sec:background}

\subsection*{Probabilistic Numerical Methods}
The field of probabilistic numerics methods frames traditional numerical tasks as problems of probabilistic inference. This approach enables the explicit modeling of uncertainty arising from limited computational resources \cite{itergp}, discretization errors \cite{pnmol}, or incomplete information \cite{exponential_probabilistic}. The book \cite{probnum} is a comprehensive introduction to the field. Probabilistic numerics has yielded Bayesian approaches to solutions of linear algebra problems \cite{pn_solver}, optimization problems \cite{pn_optimization}, and differential equations \cite{invention_of_ODE_solver}, leading to algorithms capable of not only producing a solution but also quantifying the uncertainty in that solution. Given prior beliefs about the unknown solution and a likelihood model informed by computational observations (e.g., evaluations of a differential operator), probabilistic solvers infer a posterior distribution over the solution.


\subsection*{Numerical Solvers for Differential Equations}
Traditional numerical methods for solving differential equations, such as the Runge-Kutta \cite{runge} methods, produce deterministic approximations of the solution. One can choose a trade-off between accuracy and computational efficiency \cite{butcher}, expressed by the convergence rate \cite{kanschat} of the method.

On the other hand, Probabilistic Numerical solvers represent the solution as a distribution of functions \cite{nicoThesis} parameterized by a Gaussian process \cite{gp_Rasmussen}. Instead of providing a single point estimate of the solution, these solvers return a probability distribution that encapsulates the uncertainty over the solution. Using a probabilistic solver necessitates the choice of a prior distribution over the solution, which affects the behaviour of the solver. This is not a drawback, but a feature, as the choice of prior can encode prior knowledge about the solution, such as smoothness or periodicity \cite{gp_Rasmussen}. 

\subsection*{Why Uncertainty Quantification?}

The uncertainty of a solution is often seen as a by-product of the numerical method, but it serves as an important enabler for dynamic decision-making, especially in safety-critical applications. For example, in a medical setting, it would allow doctors to manually prescribe a treatment or get third opinions if the model shows uncertainty about its diagnosis. 
At its best, it allows reasoning about the range of possible outcomes to further propagate the uncertainty through later stages of a system.
\\
Active learning \cite{active_learning} is a field in which modeling uncertainty plays a big role. Under a limited budget, one can optimize to reduce uncertainty about some quantity of interest. In machine learning, it serves to identify regions of data where models are prone to errors, and can guide data collection \cite{safeopt}. In \cite{pnmol}, the authors model the uncertainty of spatial and temporal discretization errors in probabilistic numerical solvers of ODEs, which enables prioritized discretization where it will bring the most benefit. 
To truly be able to rely on uncertainty quantification, it must be calibrated, meaning that the uncertainty should be a good estimate of the true uncertainty. Fortunately, \cite{calibrated_probabilistic} show that probabilistic solvers of ODEs do indeed provide well- calibrated uncertainty estimates.

\subsection*{Why Differential Equations?}

Differential equations (DEs) play a central role in the natural sciences as tools for modeling and understanding dynamic systems. They describing the evolution of systems over time and/or space and are commonly found in physics, biology, engineering and even finance. In physics, famously Schrödinger's equation models the quantum state of a system, and the Navier-Stokes equations model fluid flow. In the earth sciences, models of climate systems rely on multiple sets of DEs. In biology, reaction-diffusion equations describe processes like morphogenesis\footnote{This is the process that leads to biological cells having a specific shape.}. Modeling uncertainties here is also useful - an example that comes to mind is the chance-of-rain\footnote{Apparently, this is formally known as "Probability of precipitation" } in a weather forecast, or the uncertainty in the prediction of a stock price. 

\subsection*{Differential Equations in Machine Learning}
Differential equations are also finding increasing relevance in machine learning \cite{neuralode} \cite{diffrax}. Neural ODEs enable learning continuous-time models of data, offering a more flexible alternative to discrete-time approaches like the RNN \cite{rnn}. This has led to the rise of physics-informed neural networks \cite{pinn} that are trained to satisfy some specific DE. Diffusion models \cite{diffusion} and normalizing flows \cite{nflow} are examples of probabilistic generative models that leverage the language of DEs to define complex distributions.
Generally, these approaches require some form of a numerical solver of DEs, and using probabilistic solvers can provide an alternative to the standard ones if calibrated uncertainty estimates are needed.
\subsection*{Manifolds in Machine Learning}
Riemannian manifolds provide a mathematical framework for studying curved spaces, generalizing the notion of Euclidean geometry ($\reals^n$) to non-Euclidean settings. They serve also as domains for constrained optimization problems \cite{gaussian_robot}. Some data is not naturally represented well in a Euclidean space, the simplest example being angles, which wrap around. Another example is the manifold of positive semidefinite matrices \cite{pymanopt}, useful when fitting Gaussian mixture models.
The link between probabilistic numerical solvers and manifolds has been made before in \cite{hennig2014probabilistic} where the authors compute geodesics (shortest paths) on manifolds using probabilistic solvers. In machine learning, taking into account the geometry of the data can make simple models competitive \cite{hauberg2012geometric} and lead to higher interpretability \cite{arvanitidis2017latent}.

\subsection*{Why the Focus on 2-Manifolds}
When one could have chosen to work with 4-dimensional manifolds, choosing 2-dimensional manifolds can seem like a boring choice. Although the continuous mathematical description of manifolds is mature well beyond 2 dimensions, the discrete counterparts are not as well-developed\cite{craneDDG}. Triangle meshes are a common discretization of manifolds, and in 3D finite-element methods one will encounter the tetrahedral mesh. 2-dimensions are a good starting point because they can be visualized and there are lots of interesting problems to solve in 2D \cite{walk_on_stars} \cite{repulsive_curves} \cite{diffusionnet}. 


\clearpage
\section{Riemannian Manifolds and Discrete Exterior Calculus}\label{sec:manifolds}
\input{../geometry/geometry.tex}

\clearpage
\section{Building an Intrinsic Triangulation from a Metric}\label{sec:intrinsic_triangulation}
\input{../intrinsic_triangulation/intrinsic_triangulation.tex}

\clearpage
\section{Partial Differential Equations and the Method of Lines}\label{sec:pde}
\input{../pde/pde.tex}

\clearpage
\section{Probabilistic Numerical Solver for ODEs}\label{sec:prior}
\input{../prior/prior.tex}

\clearpage
\section{Applying the Solver and Laplacian Matrix}\label{sec:solver_experiments}
\input{../solver_experiments/solver_experiments.tex}

\section{Conclusion}
We have explored only few aspects of probabilistic numerical solvers on manifolds. The thesis explored only the Laplacian, but we also gave formulas for the divergence and curl, differential operators that appear, for example, in the Navier Stokes equation. Since these are linear operators too, the methods presented here should still be applicable. Probabilistic numerical fluid flow solvers on manifolds could be an interesting future research direction, especially relevant in the context of climate models. The steep cubic cost in the state-space dimension is however a drawback. This can be sidestepped by assumining spatial independence or Kronecker structure in the prior model, as in \cite{kronecker}. The methods of discrete exterior calculus also has many promising avenues for future research. The thesis only touched upon the basics, but the field is rich with applications in computer graphics and physics simulations.
\clearpage
\printbibliography

\end{document}
